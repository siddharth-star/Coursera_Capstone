{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "# Indroduction"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<font size='3px'> In recent year ther is certain increase in accident. This cause serious damage in property , parts of body and even life.So to understand and find the real cause ,we have to work on certain data and find pattern to resolve the problem or to minimise it.By this project our project is to increase the chances of safty and will be used for various industries. This analysis has multiple applications like driving suggestion ,carefull analysis of fatal accident, depending on the weather and road conditions on any given day.This also contains their employers, insurance firms, emergency and health care personal. This is the standard data published by Seattle\u2019s police.It would be great if real-time conditions can be provided to estimate the trip safeness.</font> "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Dataset"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<font size='3px'>This is an standard data set published by the Seattle Police Department, with over 194673 observations with 37 attributes collected over the last 15 years. By this huge data we have to make a effective model to prevent future accident and reduce severity, so it can be use by people for getting security and also use by companies to build a reilable system .</font> <br>\n<b><u>Attributes </u> : </b> <font size='1px'>SEVERITYCODE , X , Y , OBJECTID , INCKEY , COLDETKEY , REPORTNO , STATUS , ADDRTYPE , INTKEY , LOCATION , EXCEPTRSNCODE , EXCEPTRSNDESC , SEVERITYCODE.1 , SEVERITYDESC , COLLISIONTYPE , PERSONCOUNT , PEDCOUNT , PEDCYLCOUNT , VEHCOUNT , INCDATE , INCDTTM , JUNCTIONTYPE , SDOT_COLCODE , SDOT_COLDESC , INATTENTIONIND , UNDERINFL , WEATHER , ROADCOND , LIGHTCOND , PEDROWNOTGRNT , SDOTCOLNUM , SPEEDING , ST_COLCODE , ST_COLDESC , SEGLANEKEY , CROSSWALKKEY , HITPARKEDCAR</font>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Methodology"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<font size='3px'>In this project python is used for easily avialblity of functionality, coding is performed  on IBM watson jupiter notebook. In python data analysis is easy to perform and python also contain sufficient libery for data tranformation like Pandas, Numpy, Matplotlib, and Seaborn .The data was mostly categorical so I stuck to graphical representation to see correlation between various variables. </font>\n<br>\n<p><font size='3px'>Process have to be followed for proper prediction severity are :\n    <ul>\n        <li><u>Problem Understanding</u> : First understand the problem in user and business aspect. Check if there is already solution of this problem ,if yes then identify the modifications.Otherwise try to find goal and how to reach that goal at least cost.</li>\n        <li><u>Data Collection</u> : Collect the data from standard source or collect data youself with least missing values.</li>\n        <li><u>Data Visualization</u> : Visualise data with matplotlib or seaborn liberay . Try to form correlation with scatter plot , pair plot and heat map ,it will give which feature is important and which have to remove .It also help to identify the outliers.</li>\n        <li><u>Data Transformation</u> : This process involve make the data to satisfies for the mathematical model which have filling missing values, normalised data , try to reduce or remove outlier , remove the independent features.</li>\n        <li><u>Data Modeling</u> : This project target variable is in labeled formed , so supervised learning is used. We have to apply supervised algorithm and get more accuracy.Different algorithm is used in this project random forest , xg boost , svm .</li>\n        <li><u>Data Evaluation</u> : After apply above algorithm we have to check accuracy and have to maximise the accuracy by adjusting paramenter and checking for algorithm. this process done by f1score , confusion matrix , by creating distribution plot ,etc</li>\n    </ul>\n    \n    \n    \n<font></p>\n"
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}